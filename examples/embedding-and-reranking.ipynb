{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "EYaZH8owszUS",
   "metadata": {
    "id": "EYaZH8owszUS"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-pinecone/blob/main/examples/embedding-and-reranking.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/langchain-ai/langchain-pinecone/blob/main/examples/embedding-and-reranking.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kygEiAXls5YR",
   "metadata": {
    "id": "kygEiAXls5YR"
   },
   "source": [
    "# Embedding and Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Sxhu7Wrs8gC",
   "metadata": {
    "id": "-Sxhu7Wrs8gC"
   },
   "source": [
    "This notebook acts as an introduction to using the Pinecone vectorstore implementation in LangChain.\n",
    "\n",
    "Alongside basic usage, we also show you how to build a two-step retrieval pipeline using embedding and rerank models.\n",
    "\n",
    "Both models are hosted by Pinecone — Pinecone is _not_ solely a vector database. The service can be used to create embeddings that power your vector search, and even rerank results to enhance result precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38bbc9e",
   "metadata": {
    "id": "f38bbc9e"
   },
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a9b89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "277a9b89",
    "outputId": "acace0d9-168b-44f1-b7fa-02489eec80a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.3/469.3 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "    \"langchain-pinecone==0.2.13\" \\\n",
    "    \"datasets==4.3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce098d5",
   "metadata": {
    "id": "cce098d5"
   },
   "source": [
    "# Load dataset\n",
    "For this demo we will use jamescalam/ai-arxiv2-semantic-chunks which research paper data chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2ddd6de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2ddd6de",
    "outputId": "a23afccf-a0d1-46c0-ca55-c01d610c9963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 documents.\n",
      "Document 1: 4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Sin\n",
      "---\n",
      "\n",
      "Document 2: Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with\n",
      "---\n",
      "\n",
      "Document 3: expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download a pre-chunked dataset using the 'datasets' library\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a public dataset containing semantic chunks of AI research papers\n",
    "data = load_dataset(\n",
    "    \"jamescalam/ai-arxiv2-semantic-chunks\",\n",
    "    split=\"train[:100]\",  # Load only the first 100 samples for demo purposes\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(data)} documents.\")\n",
    "\n",
    "# Preview the first 3 documents to understand the structure\n",
    "for i, doc in enumerate(data.select(range(3))):\n",
    "    print(f\"Document {i + 1}: {doc['content'][:200]}\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bp9TuRaBtomz",
   "metadata": {
    "id": "Bp9TuRaBtomz"
   },
   "source": [
    "Now we transform the dataset to match the expected format for Pinecone and LangChain. That means mapping each document to have `id`, `page_content`, and `metadata` fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec65ff34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec65ff34",
    "outputId": "321430de-25a3-472a-eb01-35c9c7a27309"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'page_content', 'metadata'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the dataset to match the expected format for Pinecone and LangChain\n",
    "# Map each document to have 'id', 'page_content', and 'metadata' fields\n",
    "data = data.map(\n",
    "    lambda x: {\n",
    "        \"id\": x[\"id\"],\n",
    "        \"page_content\": x[\"content\"],\n",
    "        \"metadata\": {\n",
    "            \"title\": str(x[\"title\"]),\n",
    "            \"prechunk_id\": str(x[\"prechunk_id\"]),\n",
    "            \"postchunk_id\": str(x[\"postchunk_id\"]),\n",
    "            \"arxiv_id\": str(x[\"arxiv_id\"]),\n",
    "        },\n",
    "    }\n",
    ")\n",
    "# Remove columns that are no longer needed after mapping\n",
    "data = data.remove_columns(\n",
    "    [\"title\", \"content\", \"prechunk_id\", \"postchunk_id\", \"arxiv_id\", \"references\"]\n",
    ")\n",
    "data  # Display the processed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bdd95b",
   "metadata": {
    "id": "a8bdd95b"
   },
   "source": [
    "## Create Pinecone Index\n",
    "A Pinecone index is a data structure that stores vector embeddings and allows for efficient similarity search. Before you can store or query embeddings, you need to create an index with the appropriate configuration (such as dimension and metric).\n",
    "\n",
    "The following code will connect to Pinecone using your API key, check if an index with the specified name exists, and create it if necessary. This step is essential for managing and querying your vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b166a02",
   "metadata": {
    "id": "3b166a02"
   },
   "outputs": [],
   "source": [
    "# Import Pinecone classes and utilities for index management\n",
    "from pinecone import ServerlessSpec, Pinecone\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# Retrieve Pinecone API key from environment or prompt user if not set\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") or getpass.getpass(\n",
    "    \"Enter your Pinecone API key: \"\n",
    ")\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone()\n",
    "\n",
    "# Define serverless deployment specification (cloud provider and region)\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\",\n",
    "    region=\"us-west-2\",  # You can change region as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa5868",
   "metadata": {
    "id": "afaa5868"
   },
   "source": [
    "Before initializing our vector store, let's connect to a Pinecone index. If one named index_name doesn't exist, it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68979e75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68979e75",
    "outputId": "d86528da-d9ea-4d0f-f974-181fd8310db3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1024,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'dotproduct',\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"langchain-embedding-and-reranking\"\n",
    "# List all existing indexes in your Pinecone project\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "# Check if the index already exists; if not, create it\n",
    "if index_name not in existing_indexes:\n",
    "    # Create a new index with specified dimension and metric\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1024,  # Must match embedding output size\n",
    "        metric=\"dotproduct\",  # Similarity metric\n",
    "        spec=spec,\n",
    "    )\n",
    "\n",
    "# Connect to the index for further operations\n",
    "index = pc.Index(index_name)\n",
    "# View index statistics to confirm connection\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20fbe7",
   "metadata": {
    "id": "7c20fbe7"
   },
   "source": [
    "## Let's define the embedding engine\n",
    "An embedding engine converts text data into high-dimensional vectors that capture semantic meaning. These vectors are used for similarity search in Pinecone.\n",
    "\n",
    "In the next code cell, we initialize the embedding model that will be used to generate vector representations for our documents. You can choose from several supported models depending on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "838b9e2c",
   "metadata": {
    "id": "838b9e2c"
   },
   "outputs": [],
   "source": [
    "# Import PineconeEmbeddings to generate vector representations of text\n",
    "from langchain_pinecone.embeddings import PineconeEmbeddings\n",
    "\n",
    "# Initialize the embedding model (default: multilingual-e5-large)\n",
    "# You can specify a different model if needed\n",
    "embedder = PineconeEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b103e",
   "metadata": {
    "id": "b50b103e"
   },
   "source": [
    "**Tip:** Before using any embedding model, you can call `list_supported_models()` to see all available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51322f7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51322f7e",
    "outputId": "aec7edc5-23d5-4586-e3fe-7e33dd198896"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"models\": [\n",
       "        {\n",
       "            \"model\": \"llama-text-embed-v2\",\n",
       "            \"short_description\": \"A high performance dense embedding model optimized for multilingual and cross-lingual text question-answering retrieval with support for long documents (up to 2048 tokens) and dynamic embedding size (Matryoshka Embeddings).\",\n",
       "            \"type\": \"embed\",\n",
       "            \"supported_parameters\": [\n",
       "                {\n",
       "                    \"parameter\": \"input_type\",\n",
       "                    \"type\": \"one_of\",\n",
       "                    \"value_type\": \"string\",\n",
       "                    \"required\": true,\n",
       "                    \"allowed_values\": [\n",
       "                        \"query\",\n",
       "                        \"passage\"\n",
       "                    ]\n",
       "                },\n",
       "                {\n",
       "                    \"parameter\": \"truncate\",\n",
       "                    \"type\": \"one_of\",\n",
       "                    \"value_type\": \"string\",\n",
       "                    \"required\": false,\n",
       "                    \"default\": \"END\",\n",
       "                    \"allowed_values\": [\n",
       "                        \"END\",\n",
       "                        \"NONE\",\n",
       "                        \"START\"\n",
       "                    ]\n",
       "                },\n",
       "                {\n",
       "                    \"parameter\": \"dimension\",\n",
       "                    \"type\": \"one_of\",\n",
       "                    \"value_type\": \"integer\",\n",
       "                    \"required\": false,\n",
       "                    \"default\": 1024,\n",
       "                    \"allowed_values\": [\n",
       "                        384,\n",
       "                        512,\n",
       "                        768,\n",
       "                        1024,\n",
       "                        2048\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"vector_type\": \"dense\",\n",
       "            \"default_dimension\": 1024,\n",
       "            \"modality\": \"text\",\n",
       "            \"max_sequence_length\": 2048,\n",
       "            \"max_batch_size\": 96,\n",
       "            \"provider_name\": \"NVIDIA\",\n",
       "            \"supported_metrics\": [\n",
       "                \"cosine\",\n",
       "                \"dotproduct\"\n",
       "            ],\n",
       "            \"supported_dimensions\": [\n",
       "                384,\n",
       "                512,\n",
       "                768,\n",
       "                1024,\n",
       "                2048\n",
       "            ]\n",
       "        },\n",
       "        {\n",
       "            \"model\": \"multilingual-e5-large\",\n",
       "            \"short_description\": \"A high-performance dense embedding model trained on a mixture of multilingual datasets. It works well on messy data and short queries expected to return medium-length passages of text (1-2 paragraphs)\",\n",
       "            \"type\": \"embed\",\n",
       "            \"supported_parameters\": [\n",
       "                {\n",
       "                    \"parameter\": \"input_type\",\n",
       "                    \"type\": \"one_of\",\n",
       "                    \"value_type\": \"string\",\n",
       "                    \"required\": true,\n",
       "                    \"allowed_values\": [\n",
       "                        \"query\",\n",
       "                        \"passage\"\n",
       "                    ]\n",
       "                },\n",
       "                {\n",
       "                    \"parameter\": \"truncate\",\n",
       "                    \"type\": \"one_of\",\n",
       "                    \"value_type\": \"string\",\n",
       "                    \"required\": false,\n",
       "                    \"default\": \"END\",\n",
       "                    \"allowed_values\": [\n",
       "                        \"END\",\n",
       "                        \"NONE\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"vector_type\": \"dense\",\n",
       "            \"default_dimension\": 1024,\n",
       "            \"modality\": \"text\",\n",
       "            \"max_sequence_length\": 507,\n",
       "            \"max_batch_size\": 96,\n",
       "            \"provider_name\": \"Microsoft\",\n",
       "            \"supported_metrics\": [\n",
       "                \"cosine\",\n",
       "                \"euclidean\"\n",
       "            ],\n",
       "            \"supported_dimensions\": [\n",
       "                1024\n",
       "            ]\n",
       "        },\n",
       "        {\n",
       "            \"model\": \"pinecone-sparse-english-v0\",\n",
       "            \"short_description\": \"A sparse embedding model for converting text to sparse vectors for keyword or hybrid semantic/keyword search. Built on the innovations of the DeepImpact architecture.\",\n",
       "            \"type\": \"embed\",\n",
       "            \"supported_parameters\": [\n",
       "                {\n",
       "                    \"parameter\": \"input_type\",\n",
       "                    \"type\": \"one_of\",\n",
       "                    \"value_type\": \"string\",\n",
       "                    \"required\": true,\n",
       "                    \"allowed_values\": [\n",
       "                        \"query\",\n",
       "                        \"passage\"\n",
       "                    ]\n",
       "                },\n",
       "                {\n",
       "                    \"parameter\": \"truncate\",\n",
       "                    \"type\": \"one_of\",\n",
       "                    \"value_type\": \"string\",\n",
       "                    \"required\": false,\n",
       "                    \"default\": \"END\",\n",
       "                    \"allowed_values\": [\n",
       "                        \"END\",\n",
       "                        \"NONE\"\n",
       "                    ]\n",
       "                },\n",
       "                {\n",
       "                    \"parameter\": \"return_tokens\",\n",
       "                    \"type\": \"any\",\n",
       "                    \"value_type\": \"boolean\",\n",
       "                    \"required\": false,\n",
       "                    \"default\": false\n",
       "                },\n",
       "                {\n",
       "                    \"parameter\": \"max_tokens_per_sequence\",\n",
       "                    \"type\": \"one_of\",\n",
       "                    \"value_type\": \"integer\",\n",
       "                    \"required\": false,\n",
       "                    \"default\": 512,\n",
       "                    \"allowed_values\": [\n",
       "                        512,\n",
       "                        2048\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"vector_type\": \"sparse\",\n",
       "            \"modality\": \"text\",\n",
       "            \"max_sequence_length\": 512,\n",
       "            \"max_batch_size\": 96,\n",
       "            \"provider_name\": \"Pinecone\",\n",
       "            \"supported_metrics\": [\n",
       "                \"dotproduct\"\n",
       "            ]\n",
       "        }\n",
       "    ]\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all supported embedding models for PineconeEmbeddings\n",
    "PineconeEmbeddings().list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1af9dd",
   "metadata": {
    "id": "fd1af9dd"
   },
   "source": [
    "## Building an Index\n",
    "After creating a Pinecone index and initializing the embedding engine, the next step is to build a vector store. A vector store is an interface that allows you to add, search, and manage vectorized documents within your Pinecone index.\n",
    "\n",
    "The following code demonstrates how to create a vector store using the Pinecone index and embedding engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8277ee43",
   "metadata": {
    "id": "8277ee43"
   },
   "outputs": [],
   "source": [
    "# Import PineconeVectorStore to manage vector data in Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Create a vector store using the connected index and embedding engine\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35b2ad",
   "metadata": {
    "id": "ff35b2ad"
   },
   "source": [
    "## Manage vector store\n",
    "Managing a vector store involves adding, updating, and deleting documents. Once your vector store is set up, you can interact with it to store new documents, remove outdated ones, or update existing entries.\n",
    "\n",
    "The next code cell shows how to add documents to your Pinecone vector store, preparing them for efficient similarity search and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "100e06d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "100e06d3",
    "outputId": "81ece69f-1deb-4f7f-b74a-2d2c2263a419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 2401.04088#0...\n",
      "page_content: 4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jia ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#1', 'prechunk_id': '', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#1...\n",
      "page_content: Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/  ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#2', 'prechunk_id': '2401.04088#0', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#2...\n",
      "page_content: expertsâ ) to process the token and combine their output additively. This technique increases the nu ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#3', 'prechunk_id': '2401.04088#1', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#3...\n",
      "page_content: Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Pre ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#4', 'prechunk_id': '2401.04088#2', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#4...\n",
      "page_content: # 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modi ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#5', 'prechunk_id': '2401.04088#3', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#5...\n",
      "page_content: # j nâ G(x)i Â· Ei(x). i=0 Here, G(x)i denotes the n-dimensional output of the gating network for th ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#6', 'prechunk_id': '2401.04088#4', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#6...\n",
      "page_content: # 1https://mistral.ai/news/mixtral-of-experts/ 2 4096 32 128 14336 32 8 32768 32000 8 2 can increase ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#7', 'prechunk_id': '2401.04088#5', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#7...\n",
      "page_content: We compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair com ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#8', 'prechunk_id': '2401.04088#6', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#8...\n",
      "page_content: 3 Active Params MMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K 7B 44.4% 77.1% 69 ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#9', 'prechunk_id': '2401.04088#7', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#9...\n",
      "page_content: Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 7 ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#10', 'prechunk_id': '2401.04088#8', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#10...\n",
      "page_content: The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is s ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#11', 'prechunk_id': '2401.04088#9', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#11...\n",
      "page_content: 4 LLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86 ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#12', 'prechunk_id': '2401.04088#10', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#12...\n",
      "page_content: Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Ar ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#13', 'prechunk_id': '2401.04088#11', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#13...\n",
      "page_content: # 3.2 Long range performance To assess the capabilities of Mixtral to tackle long context, we evalua ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#14', 'prechunk_id': '2401.04088#12', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#14...\n",
      "page_content: BBQ is a dataset of hand-written question sets that target attested social biases against nine diffe ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#15', 'prechunk_id': '2401.04088#13', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#15...\n",
      "page_content: # Instruction Fine-tuning We train Mixtral â Instruct using supervised fine-tuning (SFT) on an instr ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#16', 'prechunk_id': '2401.04088#14', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#16...\n",
      "page_content: Figure 6: LMSys Leaderboard. (Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an A ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#17', 'prechunk_id': '2401.04088#15', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#17...\n",
      "page_content: in English often get routed through the same expert even though they involve multiple tokens. Simila ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#18', 'prechunk_id': '2401.04088#16', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#18...\n",
      "page_content: Layer 0 First choice Layer 15 Layer 31 Layer 0 First or second choice Layer 15 Layer 31 ArXiv DM Mat ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#19', 'prechunk_id': '2401.04088#17', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#19...\n",
      "page_content: Repetitions at the first layer are close to random, but are significantly higher at layers 15 and 31 ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#20', 'prechunk_id': '2401.04088#18', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#20...\n",
      "page_content: init__(self, experts//List [nn.Modutel,) | Super (V7 init assert len(experts) > 0 self. experts = nn ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#21', 'prechunk_id': '2401.04088#19', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#21...\n",
      "page_content: Tensor): inputs_squashed = inputs.view(=1, inputs) gate_logits = self.gate( inputs_squashed) weights ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#22', 'prechunk_id': '2401.04088#20', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#22...\n",
      "page_content: lying in stitt air is (A) the same (18) greater) (C) less (D)! either grea lor less depending on win ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#23', 'prechunk_id': '2401.04088#21', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#23...\n",
      "page_content: 8 # Acknowledgements We thank the CoreWeave and Scaleway teams for technical support as we trained o ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#24', 'prechunk_id': '2401.04088#22', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#24...\n",
      "page_content: Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. [3] Yonatan B ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#25', 'prechunk_id': '2401.04088#23', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#25...\n",
      "page_content: Quac: Question answering in context. arXiv preprint arXiv:1808.07036, 2018. [6] Aidan Clark, Diego D ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#26', 'prechunk_id': '2401.04088#24', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#26...\n",
      "page_content: Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10 ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#27', 'prechunk_id': '2401.04088#25', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#27...\n",
      "page_content: Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of  ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#28', 'prechunk_id': '2401.04088#26', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#28...\n",
      "page_content: [14] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phan ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#29', 'prechunk_id': '2401.04088#27', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#29...\n",
      "page_content: Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learni ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#30', 'prechunk_id': '2401.04088#28', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#30...\n",
      "page_content: Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv prep ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#31', 'prechunk_id': '2401.04088#29', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#31...\n",
      "page_content: Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:230 ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#32', 'prechunk_id': '2401.04088#30', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#32...\n",
      "page_content: [26] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversari ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#33', 'prechunk_id': '2401.04088#31', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#33...\n",
      "page_content: [28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and J ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#34', 'prechunk_id': '2401.04088#32', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#34...\n",
      "page_content: Attention is all you need. Advances in neural information processing systems, 30, 2017. [32] Rowan Z ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#35', 'prechunk_id': '2401.04088#33', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#35...\n",
      "page_content: 10 [34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizh ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#36', 'prechunk_id': '2401.04088#34', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#36...\n",
      "page_content: ArXiv Github PhilPapers. StackExchange |_| | |_| | | DM Mathematics | Gutenberg || PubMed Abstracts  ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '2401.04088#37', 'prechunk_id': '2401.04088#35', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2401.04088#37...\n",
      "page_content: Wikipedia (en) # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated assig ...\n",
      "metadata: {'arxiv_id': '2401.04088', 'postchunk_id': '', 'prechunk_id': '2401.04088#36', 'title': 'Mixtral of Experts'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#0...\n",
      "page_content: 3 2 0 2 c e D 8 2 ] G L . s c [ 1 v 8 3 2 7 1 . 2 1 3 2 : v i X r a # Fast Inference of Mixture-of-E ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#1', 'prechunk_id': '', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#1...\n",
      "page_content: With the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are l ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#2', 'prechunk_id': '2312.17238#0', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#2...\n",
      "page_content: To use these LLMs on more affordable hardware setups, one must either compress model parameters (Det ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#3', 'prechunk_id': '2312.17238#1', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#3...\n",
      "page_content: for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#4', 'prechunk_id': '2312.17238#2', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#4...\n",
      "page_content: â ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 308 ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#5', 'prechunk_id': '2312.17238#3', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#5...\n",
      "page_content: Most of these models follow traditional (dense) Transformer architecture for embeddings and attentio ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#6', 'prechunk_id': '2312.17238#4', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#6...\n",
      "page_content: 2 factorization Hsu et al. (2022), or a combination thereof. These compression types are not specifi ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#7', 'prechunk_id': '2312.17238#5', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#7...\n",
      "page_content: s parameters to be loaded. # 2.4 Hardware Setup While our analysis is not specific to any hardware s ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#8', 'prechunk_id': '2312.17238#6', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#8...\n",
      "page_content: # 3 Method In this work, we aim to systematically find the optimal way to inference modern Mixture-o ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#9', 'prechunk_id': '2312.17238#7', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#9...\n",
      "page_content: 3 Selected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a â me: a n: ee Layer  ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#10', 'prechunk_id': '2312.17238#8', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#10...\n",
      "page_content: The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap  ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#11', 'prechunk_id': '2312.17238#9', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#11...\n",
      "page_content: 4 While analyzing modern MoE models, we found that it is possible to get an accurate guess of next l ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#12', 'prechunk_id': '2312.17238#10', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#12...\n",
      "page_content: We trigger speculative expert loading immediately after the system finished loading all experts for  ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#13', 'prechunk_id': '2312.17238#11', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#13...\n",
      "page_content: 5 iy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS  ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#14', 'prechunk_id': '2312.17238#12', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#14...\n",
      "page_content: We measure WikiText2 perpliexity Merity et al. (2016), C4 perplexity Raffel et al. (2020), as well a ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#15', 'prechunk_id': '2312.17238#13', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#15...\n",
      "page_content: Green values correspond to the configurations we chose for full system evaluation. 6 Algorithm 2-bit ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#16', 'prechunk_id': '2312.17238#14', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#16...\n",
      "page_content: 1. FP16 (no quantization) 2. HQQ 4-bit with group size 64, scale group size 256 3. HQQ 3-bit with gr ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#17', 'prechunk_id': '2312.17238#15', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#17...\n",
      "page_content: Finally, when evaluating 3-bit models, we use a cloud T4 from Microsoft Azure because the free-tier  ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#18', 'prechunk_id': '2312.17238#16', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#18...\n",
      "page_content: 7 and explore how mixed quantization affects perplexity and performance on language understanding ta ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#19', 'prechunk_id': '2312.17238#17', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#19...\n",
      "page_content: # Acknowledgements Authors would like to acknowledge mobicham@ for helpful discussions on Mixtral qu ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#20', 'prechunk_id': '2312.17238#18', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#20...\n",
      "page_content: 22. IEEE Press, 2022. ISBN 9784665454445. Badri, H. and Shaji, A. Half-quadratic quantization of lar ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#21', 'prechunk_id': '2312.17238#19', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#21...\n",
      "page_content: Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXi ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#22', 'prechunk_id': '2312.17238#20', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#22...\n",
      "page_content: Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Collobert, R., ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#23', 'prechunk_id': '2312.17238#21', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#23...\n",
      "page_content: Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arX ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#24', 'prechunk_id': '2312.17238#22', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#24...\n",
      "page_content: Glam: Efficient scaling of language models with mixture-of-experts, 2022. Fedus, W., Zoph, B., and S ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#25', 'prechunk_id': '2312.17238#23', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#25...\n",
      "page_content: Frantar, E. and Alistarh, D. Qmoe: Practical sub-1-bit compression of trillion-parameter models, 202 ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#26', 'prechunk_id': '2312.17238#24', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#26...\n",
      "page_content: Google. Google colaboratory, 2023. URL https://colab.research.google.com/. Hendrycks, D., Burns, C., ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#27', 'prechunk_id': '2312.17238#25', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#27...\n",
      "page_content: Openassistant conversations â democratizing large language model alignment, 2023. Lample, G., Sablay ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#28', 'prechunk_id': '2312.17238#26', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#28...\n",
      "page_content: 8557. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers- with- ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#29', 'prechunk_id': '2312.17238#27', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#29...\n",
      "page_content: A survey. CoRR, abs/2101.09671, 2021. URL https://arxiv.org/abs/2101.09671. Lin, J., Tang, J., Tang, ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#30', 'prechunk_id': '2312.17238#28', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#30...\n",
      "page_content: 9 Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive rou ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#31', 'prechunk_id': '2312.17238#29', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#31...\n",
      "page_content: Training large neural networks with constant memory using a new execution algorithm. CoRR, abs/2002. ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#32', 'prechunk_id': '2312.17238#30', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#32...\n",
      "page_content: 67, 2020. Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S., Zhang, M., Li, D., and H ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#33', 'prechunk_id': '2312.17238#31', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#33...\n",
      "page_content: Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 20 ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#34', 'prechunk_id': '2312.17238#32', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#34...\n",
      "page_content: High-throughput generative inference of large language models with a single gpu. In International Co ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#35', 'prechunk_id': '2312.17238#33', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#35...\n",
      "page_content: W., Lu, H., Sifre, L., Maggioni, M., Alcober, F., Garrette, D., Barnes, M., Thakoor, S., Austin, J., ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#36', 'prechunk_id': '2312.17238#34', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#36...\n",
      "page_content: R., Vasudevan, V., Agrawal, S., Riesa, J., Lepikhin, D., Tanburn, R., Srinivasan, S., Lim, H., Hodki ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#37', 'prechunk_id': '2312.17238#35', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#37...\n",
      "page_content: 10 D., Sachan, D., Amplayo, R. K., Swanson, C., Petrova, D., Narayan, S., Guez, A., Brahma, S., Land ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#38', 'prechunk_id': '2312.17238#36', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#38...\n",
      "page_content: K., Agrawal, S., Mensch, A., Soparkar, K., Lenc, K., Chung, T., Pope, A., Maggiore, L., Kay, J., Jha ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#39', 'prechunk_id': '2312.17238#37', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#39...\n",
      "page_content: L., Cevey, S., Gleicher, Z., Avrahami, T., Boral, A., Srinivasan, H., Selo, V., May, R., Aisopos, K. ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#40', 'prechunk_id': '2312.17238#38', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#40...\n",
      "page_content: H., Ives, R., Hasson, Y., Li, Y., Noland, E., Cao, Y., Byrd, N., Hou, L., Wang, Q., Sottiaux, T., Pa ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#41', 'prechunk_id': '2312.17238#39', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#41...\n",
      "page_content: S., Mohiuddin, A., Muhammad, F., Miao, J., Lee, A., Vieillard, N., Potluri, S., Park, J., Davoodi, E ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#42', 'prechunk_id': '2312.17238#40', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#42...\n",
      "page_content: D., Kurylowicz, P., Velury, S., Krause, S., Hardin, C., Dixon, L., Janzer, L., Choo, K., Feng, Z., Z ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#43', 'prechunk_id': '2312.17238#41', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#43...\n",
      "page_content: J., Polozov, A., Kushman, N., Krakovna, V., Brown, S., Bateni, M., Duan, D., Firoiu, V., Thotakuri,  ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#44', 'prechunk_id': '2312.17238#42', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#44...\n",
      "page_content: H., Sanmiya, J., Gladchenko, E., Trdin, N., Sozanschi, A., Toyama, D., Rosen, E., Tavakkol, S., Xue, ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#45', 'prechunk_id': '2312.17238#43', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#45...\n",
      "page_content: A., Li, Y., Lu, T., Ittycheriah, A., Shroff, P., Sun, P., Varadarajan, M., Bahargam, S., Willoughby, ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#46', 'prechunk_id': '2312.17238#44', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#46...\n",
      "page_content: R., Aslanides, J., Vyas, N., Wicke, M., Ma, X., Bilal, T., Eltyshev, E., Balle, D., Martin, N., Cate ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#47', 'prechunk_id': '2312.17238#45', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#47...\n",
      "page_content: Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., RoziÃ¨re, B., Goyal ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '2312.17238#48', 'prechunk_id': '2312.17238#46', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.17238#48...\n",
      "page_content: 12 ...\n",
      "metadata: {'arxiv_id': '2312.17238', 'postchunk_id': '', 'prechunk_id': '2312.17238#47', 'title': 'Fast Inference of Mixture-of-Experts Language Models with Offloading'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#0...\n",
      "page_content: 3 2 0 2 c e D 9 1 ] I A . s c [ 2 v 1 1 1 1 1 . 2 1 3 2 : v i X r a # The Good, The Bad, and Why: Un ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#1', 'prechunk_id': '', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#1...\n",
      "page_content: Emotion significantly impacts our daily behaviors and interactions. While recent genera- tive AI mod ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#2', 'prechunk_id': '2312.11111#0', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#2...\n",
      "page_content: 1 # Introduction Emotion is a multifaceted psychological and physiological phenomenon that encompass ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#3', 'prechunk_id': '2312.11111#1', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#3...\n",
      "page_content: â Corresponding author: Jindong Wang. Email: jindong.wang@microsoft.com. Address: No.5 Danling Stree ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#4', 'prechunk_id': '2312.11111#2', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#4...\n",
      "page_content: inside AI models Figure 1: An overview of our research on unveiling emotions in generative AI models ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#5', 'prechunk_id': '2312.11111#3', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#5...\n",
      "page_content: 2 tors that demonstrate how emotions can impede human problem-solving, such as negative life events  ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#6', 'prechunk_id': '2312.11111#4', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#6...\n",
      "page_content: Each type contains 5 dif- ferent images that append the original textual prompts in multi-modal mode ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#7', 'prechunk_id': '2312.11111#5', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#7...\n",
      "page_content: To conclude, this paper makes the following contributions: 1. Theory-driven Method in Understanding  ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#8', 'prechunk_id': '2312.11111#6', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#8...\n",
      "page_content: 3 (a) Performance change by EmotionPrompt (>0) and EmotionAttack (<0) with human study. Semantic und ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#9', 'prechunk_id': '2312.11111#7', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#9...\n",
      "page_content: EmotionDecode (EmotionPrompt) EmotionDecode (EmotionAttack) EmotionDecode (Neutral stimuli) 10 sa Â£ ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#10', 'prechunk_id': '2312.11111#8', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#10...\n",
      "page_content: Figure 2: (a) The main results of textual and visual EmotionPrompt and EmotionAttack on gener- ative ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#11', 'prechunk_id': '2312.11111#9', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#11...\n",
      "page_content: Generative AI models understand and can be influenced by emotional stimuli. Emo- tionPrompt and Emot ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#12', 'prechunk_id': '2312.11111#10', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n",
      "Processing document 2312.11111#12...\n",
      "page_content: 2Under this metric, a score of 100 corresponds to human experts, and 0 corresponds to random guessin ...\n",
      "metadata: {'arxiv_id': '2312.11111', 'postchunk_id': '2312.11111#13', 'prechunk_id': '2312.11111#11', 'title': 'The Good, The Bad, and Why: Unveiling Emotions in Generative AI'}\n",
      "metadata: <class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['6a492517-50f7-4c7a-a010-7366fcd0d427',\n",
       " '5fc47369-c665-4049-847c-207dc575773b',\n",
       " '7a33b3f0-28d2-420f-8e55-22b16011c6a7',\n",
       " '56810ccb-2b90-4466-bbe8-b6b8f4bb9d8d',\n",
       " 'f7369f1d-ff07-4882-b729-e4a5958cd0a4',\n",
       " 'f6374c0b-5147-41a8-8373-e23703b8d687',\n",
       " 'bd9c27f0-373b-4362-a255-84e7a05ba10d',\n",
       " 'c5c4a097-2135-4e68-bee2-5073dc19f7a0',\n",
       " 'a052acf0-c241-4a40-aa81-7b0066e9a7a2',\n",
       " '1364dc05-b4de-4aee-82fd-711ec6bac5a9',\n",
       " '42cc7a97-5ef5-4be1-adf0-e49320f9dc32',\n",
       " '35a620ad-a794-4a96-9a7a-3ebdb26603b4',\n",
       " '275d9335-98dc-4ddf-8362-de616e0b932a',\n",
       " '36920e8f-dc23-4482-a0f5-9f8eba1b395f',\n",
       " '2a1631fa-f26f-433c-95b4-700991e0f044',\n",
       " '860bad44-ea46-4d23-b5b1-d2c4c8664b26',\n",
       " 'cf3236f0-040f-43ca-8ab1-5f2ecedc9e88',\n",
       " 'c9985e0e-8f96-49d7-b40c-bc0632e77783',\n",
       " '7701bd23-9382-449a-98ea-270ce7738aa2',\n",
       " '1733d9de-a806-4c25-8009-465e61f14ff4',\n",
       " 'd3145b3c-2c07-4144-9cc3-38d602b0642c',\n",
       " '40dcefe8-66a1-488c-8f43-f0510ec91957',\n",
       " '24f5f6d5-a251-4cf3-aaff-1cd53f392344',\n",
       " 'fa06c7cf-3e13-4353-a925-2ca148c25907',\n",
       " '132d9da7-bdb4-43a8-9ec3-e34475b17e1b',\n",
       " '09357929-2fc6-4b54-8df1-754e8b796e85',\n",
       " '57f2457b-d2e3-499a-af49-1dc35493d542',\n",
       " 'a0e145e6-82d6-426e-8af0-731ae08927e6',\n",
       " '272c52ee-0602-4a82-881d-87bc291e616a',\n",
       " '4b1ebea3-f7bc-437a-8774-ab848e0421a8',\n",
       " 'b522ddc5-78dd-4ef2-a8c4-f3b51543acee',\n",
       " '63ebebbd-5910-43c7-b4c5-f234c1356d2c',\n",
       " '9dfa3df8-9ff6-4afc-829e-a5976179f1f5',\n",
       " 'e608d0ea-7b7f-4ab4-b716-b8ac1cd45df6',\n",
       " '5f30371c-7c40-4b8a-b6cd-96102d02f8a2',\n",
       " 'cb180bce-7323-410c-8e06-c38be730ad4b',\n",
       " '6d88c43f-6b96-413a-9e84-9a677c21be46',\n",
       " '71cc966b-12e5-45f0-984e-e6bc1f4b9c6d',\n",
       " 'feebc11f-9d55-452b-be77-ff918fc3d942',\n",
       " 'e66a3b89-8c8d-44dc-8dc2-c08f8b7fc355',\n",
       " '41d51fa4-4eb0-43d7-956c-dbadc277517d',\n",
       " 'c6f1aaee-f9ac-43b9-8e1f-42e890c70228',\n",
       " '8a80e2bc-a664-4998-8789-930dd575216d',\n",
       " 'a93f71ad-1458-4b1b-997c-acbde79336fd',\n",
       " '04f8abd9-7ee0-416e-b24a-c493d59da446',\n",
       " '66c3601f-8563-43a0-806a-728f335d4838',\n",
       " '8da9f071-1057-419e-80cd-b0d826eb1606',\n",
       " '30509c51-ac72-42b8-b2e4-254cdc67c007',\n",
       " 'c6fdd848-f0ff-461c-8bc6-20fed6c11c3f',\n",
       " 'daed62f3-0674-4e13-b478-a453a02c16e7',\n",
       " '9078f3f4-9c80-4222-83f2-db0a048a2c49',\n",
       " 'c378bc29-9f03-4dbd-8e86-d96be7d97cf7',\n",
       " '6ec7194b-d75a-4886-96c0-676bb5635d21',\n",
       " 'fce8c38b-3b98-4d7a-8360-979ea3e6630e',\n",
       " 'b57960ee-695a-483e-b1fe-f8dccb8cc570',\n",
       " '2dc99425-4c50-4be8-abbf-e182be92d4f7',\n",
       " '6e360267-1aaf-421e-8b9b-a100466141b1',\n",
       " 'c595fad5-3620-4de1-a8e6-a122297a389e',\n",
       " '0a20a482-6935-4f99-83ae-f53ecde5da18',\n",
       " 'b6817c17-61dd-4a9a-80f7-f9a3343a1298',\n",
       " '07206007-acea-46a4-a800-c44ff3e57a4d',\n",
       " '3082e79f-5692-4d2c-9710-2e02273a0d79',\n",
       " '9abad7de-ccbf-45b9-8db7-77a222b74915',\n",
       " 'c8ccfea8-9178-42cd-845a-a28d23c3aba1',\n",
       " '1dd5c474-1c1e-4ac6-b9a7-4bd9b654519f',\n",
       " '3da8d3c7-1a49-40c4-bcd0-97ff7d28e1d6',\n",
       " '8e6c917d-0ce5-41df-bd80-556897fcf10b',\n",
       " '11de8637-d935-4eab-a163-b203ea6c6f5f',\n",
       " '2efa680b-5521-415b-85da-6229f6519c0c',\n",
       " 'e8f537ed-db82-402a-a2d8-8809d4f3042d',\n",
       " '7813288f-bdb9-459e-bed6-7c389f18554e',\n",
       " '046c21b6-4d34-4aab-ab29-0f2d7b62b8d1',\n",
       " '04adc902-68c9-497d-a50c-e4665085cf3c',\n",
       " 'cf58e495-cb0a-4e53-afb3-319c9a530cc9',\n",
       " '78b6e1d4-9e66-4c47-98e6-9f3520eaa7da',\n",
       " 'b9d37fdb-4e2d-47d7-8b6f-460ceaca6297',\n",
       " 'a8d321fb-f8be-4419-bbc5-47bee9813713',\n",
       " 'b3355eeb-5ca5-4929-9888-e0eb98072159',\n",
       " 'd9e594da-7ada-4623-a602-2e740027cee5',\n",
       " 'ee154fed-2b74-49fd-b97c-e740cb3a15e2',\n",
       " '32a2865e-b849-4ef6-8af6-e4362646ceff',\n",
       " 'a01eb886-d3e9-49b2-a7a6-77dab8b5e8d6',\n",
       " 'ab483b6b-7ad8-4040-90eb-155cd9d634a4',\n",
       " '6c29ac4f-36f8-4a64-b309-9974470be5dc',\n",
       " 'd0b1581a-8c1d-45fc-a443-9f73b05a590e',\n",
       " '3914da19-c987-49e1-88d0-db89b8379431',\n",
       " '48c8a2a7-df89-44e6-96e2-a7de147a157d',\n",
       " '99cb9f27-0f96-4ce3-9886-a9bf235c0933',\n",
       " 'eaf7957e-45f3-48d3-af82-e698190f40ca',\n",
       " '0489acb7-b98d-4942-bc5a-3f7840c63881',\n",
       " '40859427-f7e7-410c-8f32-de56f7caf53b',\n",
       " '2fe3f3eb-3776-4e78-9295-2cfc262c8c0a',\n",
       " '84aae43f-b6bb-4333-bcab-ce47c7a3c800',\n",
       " '5ec536a5-a9c9-4322-8963-66687a747677',\n",
       " '0fc3e24d-451d-4ff6-baa6-44d9cf90e50b',\n",
       " '03bfe85e-c665-4091-896a-4db2f20a3dae',\n",
       " 'e6f3660e-2ef6-4d94-8983-99379484cd73',\n",
       " 'e73aa947-ec8a-4695-b29c-34203e166846',\n",
       " 'e6a34693-eb91-4dd7-ac2d-f8faca7e12ef',\n",
       " '0c434fde-49cf-42de-87f1-39d2a6578c4c']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import utilities for document creation and unique IDs\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "for i in data:\n",
    "    # Print document details for debugging and inspection\n",
    "    print(f\"Processing document {i['id']}...\")\n",
    "    print(\n",
    "        \"page_content:\", i[\"page_content\"][:100], \"...\"\n",
    "    )  # Preview first 100 characters\n",
    "    print(\"metadata:\", i[\"metadata\"])\n",
    "    print(\"metadata:\", type(i[\"metadata\"]))\n",
    "    # Create a Document object for each entry\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content=str(data[\"page_content\"]),\n",
    "            metadata=data[\"metadata\"]\n",
    "            if isinstance(data[\"metadata\"], dict)\n",
    "            else data[\"metadata\"][0],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Generate unique IDs for each document\n",
    "uuids = [str(uuid4()) for _ in range(len(data))]\n",
    "# Add documents to the Pinecone vector store\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713cdff",
   "metadata": {
    "id": "6713cdff"
   },
   "source": [
    "## Retrieval\n",
    "Retrieval is the process of searching for documents in your vector store that are most similar to a given query. This is done using vector similarity search, which finds documents whose embeddings are closest to the query embedding.\n",
    "\n",
    "The following section demonstrates how to retrieve documents using similarity search. First, we do _without_ reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfef2cbc",
   "metadata": {
    "id": "bfef2cbc"
   },
   "outputs": [],
   "source": [
    "# Define a function to retrieve documents from the vector store using similarity search\n",
    "def get_docs(query: str, top_k: int) -> list[str]:\n",
    "    # Perform similarity search for the query\n",
    "    docs = vector_store.similarity_search(query, k=top_k)\n",
    "    print(f\"Found {len(docs)} documents for query '{query}'\")\n",
    "    response_content = []\n",
    "    for doc in docs:\n",
    "        # Format the output for each document\n",
    "        response_content.append(\n",
    "            f\"\"\"\n",
    "                Arxiv ID: {doc.metadata.get(\"arxiv_id\", \"N/A\")}\n",
    "                Title: {doc.metadata.get(\"title\", \"\")}\n",
    "                Content: {doc.page_content}\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02513ebe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02513ebe",
    "outputId": "ca74dbf5-f381-42fb-bd99-68501ab6dd1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 documents for query 'can you tell me about mistral LLM?'\n",
      "\n",
      "                Arxiv ID: 2401.04088\n",
      "                Title: Mixtral of Experts\n",
      "                Content: Column(['4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â', 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by: Table 1: Model architecture.'])\n",
      "            \n",
      "---\n",
      "\n",
      "                Arxiv ID: 2401.04088\n",
      "                Title: Mixtral of Experts\n",
      "                Content: Column(['4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â', 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by: Table 1: Model architecture.'])\n",
      "            \n",
      "---\n",
      "\n",
      "                Arxiv ID: 2401.04088\n",
      "                Title: Mixtral of Experts\n",
      "                Content: Column(['4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â', 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by: Table 1: Model architecture.'])\n",
      "            \n",
      "---\n",
      "\n",
      "                Arxiv ID: 2401.04088\n",
      "                Title: Mixtral of Experts\n",
      "                Content: Column(['4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â', 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by: Table 1: Model architecture.'])\n",
      "            \n",
      "---\n",
      "\n",
      "                Arxiv ID: 2401.04088\n",
      "                Title: Mixtral of Experts\n",
      "                Content: Column(['4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â', 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by: Table 1: Model architecture.'])\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# Example query to retrieve documents about Mistral LLM\n",
    "query = \"can you tell me about mistral LLM?\"\n",
    "# Retrieve top 5 documents using similarity search\n",
    "docs = get_docs(query, top_k=5)\n",
    "# Print the retrieved documents\n",
    "print(\"\\n---\\n\".join(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1cc89",
   "metadata": {
    "id": "7db1cc89"
   },
   "source": [
    "### Adding Reranking\n",
    "Reranking is a process that improves the relevance of search results by reordering them based on a specialized model. After retrieving documents using vector similarity, reranking models (such as BGE or Pinecone's default) analyze the results and score them according to their relevance to the query.\n",
    "\n",
    "This step is especially useful when you want to ensure that the most relevant documents appear at the top, even if the initial vector search returns many similar items.\n",
    "\n",
    "In the following code, we show how to initialize a reranker and use it to enhance your search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b73fe1d",
   "metadata": {
    "id": "3b73fe1d"
   },
   "outputs": [],
   "source": [
    "# Import PineconeRerank to improve search result relevance\n",
    "from langchain_pinecone import PineconeRerank\n",
    "\n",
    "# Initialize the reranker (default model or specify one)\n",
    "# reranker = PineconeRerank(model=\"bge-reranker-v2-m3\")\n",
    "reranker = PineconeRerank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10509c7",
   "metadata": {
    "id": "a10509c7"
   },
   "source": [
    "**Tip:** Before using any rerank model, you can call `list_supported_models()` to see all available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc243f9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc243f9a",
    "outputId": "31f0be80-2075-49ca-a1da-5694bf88066e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"models\": [\n",
       "        {\n",
       "            \"model\": \"bge-reranker-v2-m3\",\n",
       "            \"short_description\": \"A high-performance, multilingual reranking model that works well on messy data and short queries expected to return medium-length passages of text (1-2 paragraphs)\",\n",
       "            \"type\": \"rerank\",\n",
       "            \"supported_parameters\": [\n",
       "                {\n",
       "                    \"parameter\": \"truncate\",\n",
       "                    \"type\": \"one_of\",\n",
       "                    \"value_type\": \"string\",\n",
       "                    \"required\": false,\n",
       "                    \"default\": \"NONE\",\n",
       "                    \"allowed_values\": [\n",
       "                        \"END\",\n",
       "                        \"NONE\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"modality\": \"text\",\n",
       "            \"max_sequence_length\": 1024,\n",
       "            \"max_batch_size\": 100,\n",
       "            \"provider_name\": \"BAAI\",\n",
       "            \"supported_metrics\": []\n",
       "        },\n",
       "        {\n",
       "            \"model\": \"cohere-rerank-3.5\",\n",
       "            \"short_description\": \"Cohere's leading reranking model, balancing performance and latency for a wide range of enterprise search applications.\",\n",
       "            \"type\": \"rerank\",\n",
       "            \"supported_parameters\": [\n",
       "                {\n",
       "                    \"parameter\": \"max_chunks_per_doc\",\n",
       "                    \"type\": \"numeric_range\",\n",
       "                    \"value_type\": \"integer\",\n",
       "                    \"required\": false,\n",
       "                    \"default\": 3072,\n",
       "                    \"min\": 1.0,\n",
       "                    \"max\": 3072.0\n",
       "                }\n",
       "            ],\n",
       "            \"modality\": \"text\",\n",
       "            \"max_sequence_length\": 40000,\n",
       "            \"max_batch_size\": 200,\n",
       "            \"provider_name\": \"Cohere\",\n",
       "            \"supported_metrics\": []\n",
       "        },\n",
       "        {\n",
       "            \"model\": \"pinecone-rerank-v0\",\n",
       "            \"short_description\": \"A state of the art reranking model that out-performs competitors on widely accepted benchmarks. It can handle chunks up to 512 tokens (1-2 paragraphs)\",\n",
       "            \"type\": \"rerank\",\n",
       "            \"supported_parameters\": [\n",
       "                {\n",
       "                    \"parameter\": \"truncate\",\n",
       "                    \"type\": \"one_of\",\n",
       "                    \"value_type\": \"string\",\n",
       "                    \"required\": false,\n",
       "                    \"default\": \"END\",\n",
       "                    \"allowed_values\": [\n",
       "                        \"END\",\n",
       "                        \"NONE\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"modality\": \"text\",\n",
       "            \"max_sequence_length\": 512,\n",
       "            \"max_batch_size\": 100,\n",
       "            \"provider_name\": \"Pinecone\",\n",
       "            \"supported_metrics\": []\n",
       "        }\n",
       "    ]\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all supported rerank models for PineconeRerank\n",
    "PineconeRerank().list_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9c7e14d",
   "metadata": {
    "id": "d9c7e14d"
   },
   "outputs": [],
   "source": [
    "# Define a function to retrieve and rerank documents for improved relevance\n",
    "def get_docs_rerank(query: str) -> list[str]:\n",
    "    # Retrieve more documents than needed for reranking\n",
    "    docs = vector_store.similarity_search(query, k=25)\n",
    "    # Rerank the retrieved documents and select the top N\n",
    "    top5_docs = reranker.rerank(docs, query, top_n=5)\n",
    "    print(f\"Found {len(top5_docs)} documents for query '{query}'\")\n",
    "    response_content = []\n",
    "    for doc in top5_docs:\n",
    "        # Format the output for each reranked document\n",
    "        response_content.append(\n",
    "            f\"\"\"\n",
    "                Score: {doc.get(\"score\", \"0\")}\n",
    "                Arxiv ID: {doc.get(\"document\").get(\"arxiv_id\", \"N/A\")}\n",
    "                Title: {doc.get(\"document\").get(\"title\", \"\")}\n",
    "                Content: {doc.get(\"document\").get(\"text\", \"N/A\")}\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf547ec7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf547ec7",
    "outputId": "5901cbb5-632c-47be-e8dc-663c887b3d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 documents for query 'can you tell me about mistral LLM?'\n",
      "\n",
      "                Score: 0.055823144\n",
      "                Arxiv ID: 2401.04088\n",
      "                Title: Mixtral of Experts\n",
      "                Content: Column(['4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â', 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by: Table 1: Model architecture.'])\n",
      "            \n",
      "---\n",
      "\n",
      "                Score: 0.05572029\n",
      "                Arxiv ID: 2401.04088\n",
      "                Title: Mixtral of Experts\n",
      "                Content: Column(['4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â', 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by: Table 1: Model architecture.'])\n",
      "            \n",
      "---\n",
      "\n",
      "                Score: 0.05561761\n",
      "                Arxiv ID: 2401.04088\n",
      "                Title: Mixtral of Experts\n",
      "                Content: Column(['4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â', 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by: Table 1: Model architecture.'])\n",
      "            \n",
      "---\n",
      "\n",
      "                Score: 0.055412795\n",
      "                Arxiv ID: 2401.04088\n",
      "                Title: Mixtral of Experts\n",
      "                Content: Column(['4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â', 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by: Table 1: Model architecture.'])\n",
      "            \n",
      "---\n",
      "\n",
      "                Score: 0.05531065\n",
      "                Arxiv ID: 2401.04088\n",
      "                Title: Mixtral of Experts\n",
      "                Content: Column(['4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â', 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by: Table 1: Model architecture.'])\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# Example query to retrieve and rerank documents about Mistral LLM\n",
    "query = \"can you tell me about mistral LLM?\"\n",
    "# Retrieve top 5 documents using reranking\n",
    "docs = get_docs_rerank(query)\n",
    "# Print the reranked documents\n",
    "print(\"\\n---\\n\".join(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21293aa6",
   "metadata": {
    "id": "21293aa6"
   },
   "source": [
    "## Async Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DJsz0oxRwBwy",
   "metadata": {
    "id": "DJsz0oxRwBwy"
   },
   "source": [
    "This section demonstrates how to use the async API for embedding, upserting, retrieval, and reranking. Async is recommended for best performance in production and large-scale scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af52941",
   "metadata": {
    "id": "2af52941"
   },
   "source": [
    "### Async Embedding and Upsert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "puM0CCIrwD5o",
   "metadata": {
    "id": "puM0CCIrwD5o"
   },
   "source": [
    "This section demonstrates how to use Pinecone's async API for embedding and upserting documents. Async operations are recommended for production and large-scale scenarios, as they provide better performance and scalability compared to synchronous calls.\n",
    "\n",
    "- **Embedding:** Converts text data into vector representations using the selected model.\n",
    "- **Upsert:** Adds or updates documents in the Pinecone index asynchronously.\n",
    "\n",
    "You should use async workflows when working with large datasets or when you need non-blocking operations in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c7ee0a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c7ee0a9",
    "outputId": "701d068f-8d5f-4aca-89b2-585b86e4730e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deletion_protection': 'disabled',\n",
      " 'dimension': 1024,\n",
      " 'host': 'langchain-embedding-and-reranking-async-swepyyp.svc.apw5-4e34-81fa.pinecone.io',\n",
      " 'metric': 'dotproduct',\n",
      " 'name': 'langchain-embedding-and-reranking-async',\n",
      " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      " 'status': {'ready': True, 'state': 'Ready'},\n",
      " 'tags': None,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "from pinecone import PineconeAsyncio\n",
    "\n",
    "index_name = \"langchain-embedding-and-reranking-async\"\n",
    "\n",
    "# Use async context manager to interact with Pinecone\n",
    "async with PineconeAsyncio() as pc:\n",
    "    # Create index if it does not exist\n",
    "    if not await pc.has_index(index_name):\n",
    "        await pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1024,\n",
    "            metric=\"dotproduct\",\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-west-2\"\n",
    "            ),\n",
    "        )\n",
    "    # Get index description and host info\n",
    "    pc_desc = await pc.describe_index(name=index_name)\n",
    "    print(pc_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKNJBHkUwhaK",
   "metadata": {
    "id": "PKNJBHkUwhaK"
   },
   "source": [
    "We use the async context manager for index operations like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "tA7Lls-Qwez7",
   "metadata": {
    "id": "tA7Lls-Qwez7"
   },
   "outputs": [],
   "source": [
    "async with pc.IndexAsyncio(host=pc_desc.host) as idx:\n",
    "    aembedder = PineconeEmbeddings()\n",
    "    vector_store = PineconeVectorStore(index=idx, embedding=aembedder)\n",
    "    documents = []\n",
    "    for i in data:\n",
    "        # Create Document objects for each entry\n",
    "        documents.append(\n",
    "            Document(\n",
    "                page_content=str(i[\"page_content\"]),\n",
    "                metadata=i[\"metadata\"]\n",
    "                if isinstance(i[\"metadata\"], dict)\n",
    "                else i[\"metadata\"][0],\n",
    "            )\n",
    "        )\n",
    "    # Generate unique IDs and upsert documents asynchronously\n",
    "    uuids = [str(uuid4()) for _ in range(len(data))]\n",
    "    # Use async context manager for vector_store to ensure proper session cleanup\n",
    "    async with vector_store:\n",
    "        await vector_store.aadd_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44033008",
   "metadata": {
    "id": "44033008"
   },
   "source": [
    "### Async Retrieval and Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xCZdxMEiw0cR",
   "metadata": {
    "id": "xCZdxMEiw0cR"
   },
   "source": [
    "Now let's see how to retrieve documents and apply reranking using Pinecone's async client. Async retrieval is ideal for applications that require high throughput and responsiveness while avoiding blocking a full thread — making this ideal for agentic AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "728310b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "728310b1",
    "outputId": "931caf8d-a491-4685-aefe-ea3829960911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 documents for query 'can you tell me about mistral LLM?'\n",
      "\n",
      "                    Arxiv ID: 2401.04088\n",
      "                    Title: Mixtral of Experts\n",
      "                    Content: We compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow: â ¢ Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30] World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19] â ¢ Reading Comprehension (0-shot): BoolQ [7], QuAC [5] â ¢ Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4 â ¢ Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot) â ¢ Popular aggregated results: MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34] (3-5-shot, English multiple-choice questions only) 80 SE Mistral 78 = LLaMA27B = Sl LLaMA134B, jam Mistral 78 = LlaMA27B Ss LLAMA 1348, cee Mixtral 8x78 Sm LLaMA213BÂ° mmm LLaMA2 70B je Mixtral 8x78 mm LlaMA2138 lm LLaMA2 708 70 50 60 50 20 40 10 BH Code MMU Knowledge Reasoning â Comprehension AGI Eval Math â Accuracy (%) Figure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks. All models were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or matches Llama 2 70B on all benchmarks. In particular, it is vastly superior in mathematics and code generation.\n",
      "                \n",
      "---\n",
      "\n",
      "                    Arxiv ID: 2401.04088\n",
      "                    Title: Mixtral of Experts\n",
      "                    Content: We compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow: â ¢ Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30] World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19] â ¢ Reading Comprehension (0-shot): BoolQ [7], QuAC [5] â ¢ Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4 â ¢ Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot) â ¢ Popular aggregated results: MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34] (3-5-shot, English multiple-choice questions only) 80 SE Mistral 78 = LLaMA27B = Sl LLaMA134B, jam Mistral 78 = LlaMA27B Ss LLAMA 1348, cee Mixtral 8x78 Sm LLaMA213BÂ° mmm LLaMA2 70B je Mixtral 8x78 mm LlaMA2138 lm LLaMA2 708 70 50 60 50 20 40 10 BH Code MMU Knowledge Reasoning â Comprehension AGI Eval Math â Accuracy (%) Figure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks. All models were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or matches Llama 2 70B on all benchmarks. In particular, it is vastly superior in mathematics and code generation.\n",
      "                \n",
      "---\n",
      "\n",
      "                    Arxiv ID: 2312.17238\n",
      "                    Title: Fast Inference of Mixture-of-Experts Language Models with Offloading\n",
      "                    Content: Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator. # 2.2 Post-training Quantization of LLMs A natural way to circumvent this is to reduce the model size through quantization (Nagel et al., 2020; Gholami et al., 2021; Frantar et al., 2022), sparsification Frantar & Alistarh (2023a); Ma et al. (2023), 2https://github.com/dvmazur/mixtral-offloading 3https://huggingface.co/google/switch-c-2048, released in November 15th, 2022\n",
      "                \n",
      "---\n",
      "\n",
      "                    Arxiv ID: 2312.17238\n",
      "                    Title: Fast Inference of Mixture-of-Experts Language Models with Offloading\n",
      "                    Content: Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator. # 2.2 Post-training Quantization of LLMs A natural way to circumvent this is to reduce the model size through quantization (Nagel et al., 2020; Gholami et al., 2021; Frantar et al., 2022), sparsification Frantar & Alistarh (2023a); Ma et al. (2023), 2https://github.com/dvmazur/mixtral-offloading 3https://huggingface.co/google/switch-c-2048, released in November 15th, 2022\n",
      "                \n",
      "---\n",
      "\n",
      "                    Arxiv ID: 2401.04088\n",
      "                    Title: Mixtral of Experts\n",
      "                    Content: Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "# Async example: retrieval and reranking for best performance\n",
    "# Create a managed PineconeAsyncio client for the reranker to avoid unclosed client sessions\n",
    "async with PineconeAsyncio() as pc_rerank:\n",
    "    async with pc.IndexAsyncio(host=pc_desc.host) as idx:\n",
    "        aembedder = PineconeEmbeddings()\n",
    "        # Pass the managed async client to PineconeRerank to ensure proper cleanup\n",
    "        reranker = PineconeRerank(async_client=pc_rerank)\n",
    "        vector_store = PineconeVectorStore(index=idx, embedding=aembedder)\n",
    "        query = \"can you tell me about mistral LLM?\"\n",
    "        # Use async context manager for vector_store to ensure proper session cleanup\n",
    "        async with vector_store:\n",
    "            # Retrieve documents asynchronously\n",
    "            docs = await vector_store.asimilarity_search(query, k=25)\n",
    "            # Rerank the retrieved documents asynchronously\n",
    "            top5_docs = await reranker.arerank(docs, query, top_n=5)\n",
    "            print(f\"Found {len(top5_docs)} documents for query '{query}'\")\n",
    "            response_content = []\n",
    "            for doc in top5_docs:\n",
    "                # Format and print each document's details\n",
    "                response_content.append(\n",
    "                    f\"\"\"\n",
    "                    Arxiv ID: {doc[\"document\"].get(\"arxiv_id\", \"N/A\")}\n",
    "                    Title: {doc[\"document\"].get(\"title\", \"\")}\n",
    "                    Content: {doc[\"document\"].get(\"text\")}\n",
    "                \"\"\"\n",
    "                )\n",
    "            print(\"\\n---\\n\".join(response_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x_imnU-gw5ct",
   "metadata": {
    "id": "x_imnU-gw5ct"
   },
   "source": [
    "That covers everything we need for using the Pinecone vectorstore in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZG47GdgU2PYn",
   "metadata": {
    "id": "ZG47GdgU2PYn"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pine-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
