{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-pinecone/blob/main/examples/async-vectorstore.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/langchain-ai/langchain-pinecone/blob/main/examples/async-vectorstore.ipynb)"
      ],
      "metadata": {
        "id": "23VkGOqyrzAh"
      },
      "id": "23VkGOqyrzAh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx-gjf42pM4f"
      },
      "source": [
        "# Pinecone VectorStore Async Demo"
      ],
      "id": "Rx-gjf42pM4f"
    },
    {
      "cell_type": "markdown",
      "id": "3a87a232",
      "metadata": {
        "id": "3a87a232"
      },
      "source": [
        "This notebook demonstrates two ways to manage the async Pinecone client that backs `PineconeVectorStore`:\n",
        "\n",
        "1. Letting the store handle connections automatically by using `async with` (no manual close).\n",
        "2. Closing the session yourself with `await store.aclose()` when you want deterministic cleanup.\n",
        "\n",
        "Each section below walks through one of these approaches so you can run the cells and observe that the store no longer throws `RuntimeError: Session is closed` after back-to-back async calls.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5476cf73",
      "metadata": {
        "id": "5476cf73"
      },
      "source": [
        "## Prerequisites\n",
        "- Install the project dependencies (see the repository README on local, or run the next cell in colab).\n",
        "- Create or reuse a Pinecone serverless index that matches the dimensionality of your embeddings.\n",
        "- Export the credentials before running: `export PINECONE_API_KEY=...` and optionally `export PINECONE_INDEX_NAME=...`.\n",
        "- Provide an embedding model; this example uses `langchain-openai` but you can swap in any `Embeddings` implementation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \\\n",
        "    \"langchain-pinecone==0.2.13rc1\" \\\n",
        "    \"langchain-openai==1.0.1\""
      ],
      "metadata": {
        "id": "sV7bz9kzpU1T",
        "outputId": "e4e7c692-f013-4aa2-9eff-65776657eb22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sV7bz9kzpU1T",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.3/469.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.2 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decd0762",
      "metadata": {
        "id": "decd0762"
      },
      "source": [
        "## Initialize Embeddings\n",
        "\n",
        "We begin by initializing our OpenAI embedding model, this does require an `OPENAI_API_KEY` to be set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a7192937",
      "metadata": {
        "id": "a7192937",
        "outputId": "291262fa-0517-4c7e-9e5f-3583855ebe81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
        "    or getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# Demo content to upsert and query\n",
        "TEXTS = [\n",
        "    \"Pinecone is a vector database built for production workloads.\",\n",
        "    \"LangChain integrates with Pinecone for semantic search use cases.\",\n",
        "    \"Async workflows let you reuse Pinecone connections efficiently.\",\n",
        "]\n",
        "METADATAS = [{\"source\": \"demo\", \"idx\": idx} for idx, _ in enumerate(TEXTS)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6e8bee",
      "metadata": {
        "id": "ea6e8bee"
      },
      "source": [
        "Note, the snippets in `TEXTS` act as stand-ins for your own documents. Feel free to replace them with any list of strings and matching metadata before running the demos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0bd557",
      "metadata": {
        "id": "4f0bd557"
      },
      "source": [
        "## Initialize the Index\n",
        "\n",
        "A vectir index is a data structure that stores vector embeddings and allows for efficient similarity search. Before you can store or query embeddings, you need to create an index with the appropriate configuration (such as dimension and metric).\n",
        "\n",
        "The following code will connect to Pinecone using your API key - you can get a [free API key here](https://app.pinecone.io). Check if an index with the specified name exists, and create it if necessary. This step is essential for managing and querying your vector data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "99b67979",
      "metadata": {
        "id": "99b67979",
        "outputId": "37919bf1-75fd-47eb-ae85-70dc49ac118e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Pinecone API key here: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from pinecone import ServerlessSpec, Pinecone\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") \\\n",
        "    or getpass(\"Enter your Pinecone API key here: \")\n",
        "\n",
        "index_name = \"langchain-async-vectorstore\"\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pc = Pinecone()\n",
        "\n",
        "# Define serverless deployment specification (cloud provider and region)\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\",\n",
        "    region=\"us-west-2\",  # You can change region as needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f906f9e6",
      "metadata": {
        "id": "f906f9e6",
        "outputId": "24b8dd78-9f2b-4abb-fe26-20d989449b71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'dotproduct',\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# List all existing indexes in your Pinecone project\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "# Check if the index already exists; if not, create it\n",
        "if index_name not in existing_indexes:\n",
        "    # Create a new index with specified dimension and metric\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # Must match embedding model size\n",
        "        metric=\"dotproduct\",  # Similarity metric\n",
        "        spec=spec,\n",
        "    )\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)\n",
        "# View index statistics to confirm connection\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "432ecad3",
      "metadata": {
        "id": "432ecad3"
      },
      "source": [
        "## Scenario 1 – Use `async with` (no explicit close)\n",
        "The context manager keeps a single client session open for the duration of the block and then closes it automatically when the block exits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c0a6e2e",
      "metadata": {
        "id": "9c0a6e2e"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "async def async_with_demo() -> None:\n",
        "    \"\"\"Demonstrate the async context manager to reuse a single Pinecone session.\"\"\"\n",
        "    vectorstore = PineconeVectorStore(\n",
        "        index_name=index_name,\n",
        "        embedding=embedding,\n",
        "        namespace=\"async-demo\",\n",
        "    )\n",
        "\n",
        "    print(\"Entering 'async with' block — a single aiohttp session backs all operations.\")\n",
        "    async with vectorstore:\n",
        "        ids = await vectorstore.aadd_texts(TEXTS, metadatas=METADATAS)\n",
        "        print(f\"Upserted {len(ids)} vectors\")\n",
        "\n",
        "        print(\"First similarity search inside the context.\")\n",
        "        results = await vectorstore.asimilarity_search(\"pinecone async\", k=2)\n",
        "        for doc in results:\n",
        "            print(f\"  {doc.id}: {doc.page_content} -> {doc.metadata}\")\n",
        "\n",
        "        print(\"Second similarity search reuses the same session (no reconnect).\")\n",
        "        extra = await vectorstore.asimilarity_search(\"LangChain\", k=1)\n",
        "        for doc in extra:\n",
        "            print(f\"  {doc.id}: {doc.page_content} -> {doc.metadata}\")\n",
        "\n",
        "        await vectorstore.adelete(ids=ids)\n",
        "        print(\"Cleaned up vectors while still inside the context.\")\n",
        "\n",
        "    print(\"Context exited — session closed automatically.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e70a1f2f",
      "metadata": {
        "id": "e70a1f2f",
        "outputId": "10921a10-4d0e-4c1c-cf8d-42fdaaa86955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entering 'async with' block — a single aiohttp session backs all operations.\n",
            "Upserted 3 vectors\n",
            "First similarity search inside the context.\n",
            "  0aa160bd-134a-4d4a-a0fd-8f0066e48caf: Pinecone is a vector database built for production workloads. -> {'idx': 0.0, 'source': 'demo'}\n",
            "  4a84582b-17e2-4bf6-9395-43f15822306f: Async workflows let you reuse Pinecone connections efficiently. -> {'idx': 2.0, 'source': 'demo'}\n",
            "Second similarity search reuses the same session (no reconnect).\n",
            "  dedd230d-93a8-4199-9ae8-12a4a15e6013: LangChain integrates with Pinecone for semantic search use cases. -> {'idx': 1.0, 'source': 'demo'}\n",
            "Cleaned up vectors while still inside the context.\n",
            "Context exited — session closed automatically.\n"
          ]
        }
      ],
      "source": [
        "# Run the async-context-manager demo\n",
        "await async_with_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e34504c",
      "metadata": {
        "id": "7e34504c"
      },
      "source": [
        "## Scenario 2 – Sequential calls without a context manager\n",
        "Even without `async with`, the vector store now rebuilds the async client whenever a previous session has been closed. The loop below runs multiple add/search/delete cycles back-to-back without calling `aclose()` manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5772f4ae",
      "metadata": {
        "id": "5772f4ae"
      },
      "outputs": [],
      "source": [
        "async def automatic_refresh_demo() -> None:\n",
        "    vectorstore = PineconeVectorStore(\n",
        "        index_name=index_name,\n",
        "        embedding=embedding,\n",
        "        namespace=\"auto-refresh-demo\",\n",
        "    )\n",
        "\n",
        "    for run in range(2):\n",
        "        print(f\"Run {run + 1}: adding texts without an outer context manager.\")\n",
        "        payload = [f\"Pinecone auto refresh demo {run}\"]\n",
        "        metadata = [{\"source\": \"auto\", \"run\": run}]\n",
        "        ids = await vectorstore.aadd_texts(payload, metadatas=metadata)\n",
        "        print(f\"  Upserted ids: {ids}\")\n",
        "\n",
        "        results = await vectorstore.asimilarity_search(\"pinecone\", k=1)\n",
        "        for doc in results:\n",
        "            print(f\"  Search hit: {doc.page_content} -> {doc.metadata}\")\n",
        "\n",
        "        await vectorstore.adelete(ids=ids)\n",
        "        print(\"  Deleted vectors; the next loop iteration will reopen a fresh session automatically.\")\n",
        "\n",
        "    print(\"Finished sequential runs without ever calling aclose().\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "97627038",
      "metadata": {
        "id": "97627038",
        "outputId": "02276a8f-ab69-4f37-fa8c-436e48b2ff55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1: adding texts without an outer context manager.\n",
            "  Upserted ids: ['ff18e03f-1e9a-4d70-a08c-1417ced2e78b']\n",
            "  Search hit: Pinecone auto refresh demo 0 -> {'run': 0.0, 'source': 'auto'}\n",
            "  Deleted vectors; the next loop iteration will reopen a fresh session automatically.\n",
            "Run 2: adding texts without an outer context manager.\n",
            "  Upserted ids: ['414385eb-97cb-416b-b46d-cb65c890a4ac']\n",
            "  Search hit: Pinecone auto refresh demo 1 -> {'run': 1.0, 'source': 'auto'}\n",
            "  Deleted vectors; the next loop iteration will reopen a fresh session automatically.\n",
            "Finished sequential runs without ever calling aclose().\n"
          ]
        }
      ],
      "source": [
        "# Run the sequential demo without an explicit context manager\n",
        "await automatic_refresh_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c4af4c5",
      "metadata": {
        "id": "8c4af4c5"
      },
      "source": [
        "## Scenario 3 – Explicitly close the async session\n",
        "Call `await store.aclose()` when you want deterministic cleanup after a set of operations (for example before handing the store to another task). The store can still be reused afterwards because it will lazily build a new session on the next async call.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "70455074",
      "metadata": {
        "id": "70455074"
      },
      "outputs": [],
      "source": [
        "async def manual_close_demo() -> None:\n",
        "    \"\"\"Show explicit lifecycle management with aclose().\"\"\"\n",
        "    vectorstore = PineconeVectorStore(\n",
        "        index_name=index_name,\n",
        "        embedding=embedding,\n",
        "        namespace=\"manual-demo\",\n",
        "    )\n",
        "    ids = await vectorstore.aadd_texts(TEXTS[:1], metadatas=METADATAS[:1])\n",
        "    print(f\"Added initial ids: {ids}\")\n",
        "    try:\n",
        "        results = await vectorstore.asimilarity_search(\"pinecone\", k=1)\n",
        "        for doc in results:\n",
        "            print(f\"  Search hit: {doc.page_content} -> {doc.metadata}\")\n",
        "    finally:\n",
        "        await vectorstore.adelete(ids=ids)\n",
        "        print(\"Deleted initial vectors; calling aclose() to release the session.\")\n",
        "        await vectorstore.aclose()\n",
        "\n",
        "    print(\"Session closed. The next call recreates the client lazily.\")\n",
        "    follow_up_metadata = [{\"source\": \"manual\", \"stage\": \"follow-up\"}]\n",
        "    follow_up_ids = await vectorstore.aadd_texts(TEXTS[1:2], metadatas=follow_up_metadata)\n",
        "    print(f\"Added follow-up ids: {follow_up_ids}\")\n",
        "    try:\n",
        "        follow_up_results = await vectorstore.asimilarity_search(\"langchain\", k=1)\n",
        "        for doc in follow_up_results:\n",
        "            print(f\"  Follow-up hit: {doc.page_content} -> {doc.metadata}\")\n",
        "    finally:\n",
        "        await vectorstore.adelete(ids=follow_up_ids)\n",
        "        await vectorstore.aclose()\n",
        "        print(\"Explicit close called again to tidy up.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cd9451d9",
      "metadata": {
        "id": "cd9451d9",
        "outputId": "4f3ab36d-0174-4686-8bd8-2e9eb07b842f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added initial ids: ['5ad0dc35-63c9-410d-a8b5-9fea283d0c92']\n",
            "  Search hit: Pinecone is a vector database built for production workloads. -> {'idx': 0.0, 'source': 'demo'}\n",
            "Deleted initial vectors; calling aclose() to release the session.\n",
            "Session closed. The next call recreates the client lazily.\n",
            "Added follow-up ids: ['24a3d951-6e7a-4367-aa88-70847bd878c6']\n",
            "  Follow-up hit: LangChain integrates with Pinecone for semantic search use cases. -> {'source': 'manual', 'stage': 'follow-up'}\n",
            "Explicit close called again to tidy up.\n"
          ]
        }
      ],
      "source": [
        "# Run the explicit close demo\n",
        "await manual_close_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "433d193c",
      "metadata": {
        "id": "433d193c"
      },
      "source": [
        "## Notes\n",
        "- Replace the demo texts and metadata with your own dataset to mirror production behaviour.\n",
        "- `async with PineconeVectorStore(...)` keeps one HTTP session open across the block and closes it automatically on exit.\n",
        "- Without an `async with` block, each call now reinitialises the session whenever the previous one has been closed, so you can run back-to-back async operations safely.\n",
        "- `await store.aclose()` is still useful when you want deterministic cleanup between batches or before handing the store to another component.\n",
        "- If you run these cells multiple times, consider changing the namespace or cleaning up vectors to avoid duplicate data.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pine-test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}